---
layout: post
title: "Vocab Length Transformer"
date: 2023-03-01
category: research
author: "Tong Guo"
description: "Vocab Length Transformer"
---
# Vocab Length Transformer

### Abstract

We propose Vocab Length Transformer for large language model, especially for pretraining. 
Vocab Length Transformer encode the all the tokens as additional part for the origin text input.
We think that pretraining is mainly storing the words-words relationships, so we design the all-token attention-matrix part for the origin transformer.



### The Model
